library(topicmodels)
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(1,2,3,500,1000)
nstart <- 5
#Number of topics
k <- 5
library(slam)
library(tm)
library(stringr)
comments <- read.csv(file.choose(), header = T) #amex_comments.csv file
messages <- comments$message
messages <- gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
myCorpus <- Corpus(VectorSource(messages)) # create corpus
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords())
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
dataframe <- data.frame(text = unlist(sapply(clean_corpus, `[`, "content")), stringsAsFactors = F)
# remove stop words and nonsensical words after examining the corpus
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu",
"uua", "udu", "uau","ucuuu", "uuuu", "uuu",
"nnnn", "uuauub","uuauu","ueuu","can", "will",
"get", "amex", "card", "american", "express",
"uub", "also", "just", "didnt", "till", "eue",
"ufu", "since", "now", "let", "using", "hii",
"make", "even", "dont", "sure", "uu", "hi",
"th", "oh", "uuucu", "ne", "ive", "ur", "ki",
"ko", "uuub", "uubu"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
dtm <- DocumentTermMatrix(clean_corpus, control = list(wordLengths = c(2, 15)))
library(topicmodels)
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(1,2,3,500,1000)
nstart <- 5
#Number of topics
k <- 5
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method = "Gibbs",
control = list(nstart = nstart, seed = seed,
best = best, burnin = burnin,
iter = iter, thin = thin))
row_total = apply(dtm, 1, sum)
dtm.new = dtm[row_total>0,]
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm.new, k, method = "Gibbs",
control = list(nstart = nstart, seed = seed,
best = best, burnin = burnin,
iter = iter, thin = thin))
ldaOut <-LDA(dtm.new, k, method = "Gibbs",
control = list(nstart = nstart, seed = seed, burnin = burnin,
iter = iter, thin = thin))
ldaOut.topics <- as.matrix(topics(ldaOut))
ldaOut.topics
#top 5 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,5))
ldaOut.terms
topicProbabilities <- as.data.frame(ldaOut@gamma)
topicProbabilities
head(topicProbabilities)
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(1, 2, 3, 500, 1000)
nstart <- 5
#Number of topics
k <- 5
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm.new, k, method = "Gibbs",
control = list(nstart = nstart,
seed = seed,
burnin = burnin,
iter = iter,
thin = thin))
ldaOut.topics <- as.matrix(topics(ldaOut))
#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
head(topicProbabilities)
#top 5 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,5))
ldaOut.terms
