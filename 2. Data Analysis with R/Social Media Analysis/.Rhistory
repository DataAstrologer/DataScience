messages = gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3] # top 3 cleaned messages
myCorpus <- Corpus(VectorSource(messages)) # create corpus
inspect(myCorpus [1:3]) # inspect first 3 elements of the corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words and nonsensical words after examining the corpus (shown below)
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords(), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
# Examine the corpus
dataframe <- data.frame(text=unlist(sapply(clean_corpus, `[`, "content")), stringsAsFactors=F)
View(head(dataframe, 15))
# remove words having less than 3 letters and greater than 15 letters
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(3, 15)))
library(RColorBrewer)
library(wordcloud)
mat <- as.matrix(tdm)
# calculate the frequency of each word and sort
word.freq <- sort(rowSums(mat), decreasing = T)
# colors
pal <- brewer.pal(5, "Dark2")
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 5,
random.order = F, colors = pal)
wordcloud(words = names(word.freq),
freq = word.freq,
min.freq = 5,
random.order = T,
colors = pal)
wordcloud(words = names(word.freq),
freq = word.freq,
min.freq = 5,
random.order = F,
colors = pal)
messages <- comments$message
# remove non-alphanumeric characters first
messages = gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3] # top 3 cleaned messages
myCorpus <- Corpus(VectorSource(messages)) # create corpus
inspect(myCorpus [1:3]) # inspect first 3 elements of the corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words and nonsensical words after examining the corpus (shown below)
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords(), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get", "amex", "card", "american", "express"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
# Examine the corpus
dataframe <- data.frame(text=unlist(sapply(clean_corpus, `[`, "content")), stringsAsFactors=F)
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(3, 15)))
mat <- as.matrix(tdm)
# calculate the frequency of each word and sort
word.freq <- sort(rowSums(mat), decreasing = T)
# colors
pal <- brewer.pal(5, "Dark2")
# show wordcloud
wordcloud(words = names(word.freq),
freq = word.freq,
min.freq = 5,
random.order = F,
colors = pal)
messages <- comments$message
# remove non-alphanumeric characters first
messages = gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3] # top 3 cleaned messages
myCorpus <- Corpus(VectorSource(messages)) # create corpus
inspect(myCorpus [1:3]) # inspect first 3 elements of the corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words and nonsensical words after examining the corpus (shown below)
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get", "amex", "card", "american", "express", "uub"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(3, 15)))
mat <- as.matrix(tdm)
# calculate the frequency of each word and sort
word.freq <- sort(rowSums(mat), decreasing = T)
# colors
pal <- brewer.pal(5, "Dark2")
# show wordcloud
wordcloud(words = names(word.freq),
freq = word.freq,
min.freq = 5,
random.order = F,
colors = pal)
messages <- comments$message
# remove non-alphanumeric characters first
messages = gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3] # top 3 cleaned messages
myCorpus <- Corpus(VectorSource(messages)) # create corpus
inspect(myCorpus [1:3]) # inspect first 3 elements of the corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words and nonsensical words after examining the corpus (shown below)
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get", "amex", "card", "american", "express", "uub", "also", "just", "didnt", "till", "eue", "ufu", "since", "now", "let", "using", "hii","make"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
mat <- as.matrix(tdm)
# calculate the frequency of each word and sort
word.freq <- sort(rowSums(mat), decreasing = T)
# colors
pal <- brewer.pal(5, "Dark2")
# show wordcloud
wordcloud(words = names(word.freq),
freq = word.freq,
min.freq = 5,
random.order = F,
colors = pal)
messages <- comments$message
# remove non-alphanumeric characters first
messages = gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3] # top 3 cleaned messages
myCorpus <- Corpus(VectorSource(messages)) # create corpus
inspect(myCorpus [1:3]) # inspect first 3 elements of the corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words and nonsensical words after examining the corpus (shown below)
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get", "amex", "card", "american", "express", "uub", "also", "just", "didnt", "till", "eue", "ufu", "since", "now", "let", "using", "hii","make", "even", "dont", "sure"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(2, 15)))
mat <- as.matrix(tdm)
word.freq <- sort(rowSums(mat), decreasing = T)
wordcloud(words = names(word.freq),
freq = word.freq,
min.freq = 5,
random.order = F,
colors = pal)
messages <- comments$message
# remove non-alphanumeric characters first
messages = gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3] # top 3 cleaned messages
myCorpus <- Corpus(VectorSource(messages)) # create corpus
inspect(myCorpus [1:3]) # inspect first 3 elements of the corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words and nonsensical words after examining the corpus (shown below)
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get", "amex", "card", "american", "express", "uub", "also", "just", "didnt", "till", "eue", "ufu", "since", "now", "let", "using", "hii","make", "even", "dont", "sure", "uu", "hi", "th", "oh"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
# Examine the corpus
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(2, 15)))
mat <- as.matrix(tdm)
word.freq <- sort(rowSums(mat), decreasing = T)
# colors
pal <- brewer.pal(5, "Dark2")
# show wordcloud
wordcloud(words = names(word.freq),
freq = word.freq,
min.freq = 10,
random.order = F,
colors = pal)
messages <- comments$message
# remove non-alphanumeric characters first
messages = gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3] # top 3 cleaned messages
myCorpus <- Corpus(VectorSource(messages)) # create corpus
inspect(myCorpus [1:3]) # inspect first 3 elements of the corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words and nonsensical words after examining the corpus (shown below)
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get", "amex", "card", "american", "express", "uub", "also", "just", "didnt", "till", "eue", "ufu", "since", "now", "let", "using", "hii","make", "even", "dont", "sure", "uu", "hi", "th", "oh", "uuucu", "ne", "ive", "ur"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(2, 15)))
mat <- as.matrix(tdm)
word.freq <- sort(rowSums(mat), decreasing = T)
pal <- brewer.pal(5, "Dark2")
wordcloud(words = names(word.freq),
freq = word.freq,
min.freq = 10,
random.order = F,
colors = pal)
messages <- comments$message
# remove non-alphanumeric characters first
messages = gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3] # top 3 cleaned messages
myCorpus <- Corpus(VectorSource(messages)) # create corpus
inspect(myCorpus [1:3]) # inspect first 3 elements of the corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words and nonsensical words after examining the corpus (shown below)
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get", "amex", "card", "american", "express", "uub", "also", "just", "didnt", "till", "eue", "ufu", "since", "now", "let", "using", "hii","make", "even", "dont", "sure", "uu", "hi", "th", "oh", "uuucu", "ne", "ive", "ur", "ki", "ko", "uuub", "uubu"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
# Examine the corpus
dataframe <- data.frame(text=unlist(sapply(clean_corpus, `[`, "content")), stringsAsFactors=F)
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(2, 15)))
mat <- as.matrix(tdm)
word.freq <- sort(rowSums(mat), decreasing = T)
wordcloud(words = names(word.freq),
freq = word.freq,
min.freq = 10,
random.order = F,
colors = pal)
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
install.packages("stringr")
library(plyr)
library(stringr)
afinn_list <- read.delim(file.choose(), header = FALSE, stringsAsFactors = FALSE) # import AFINN-111.txt file (check data folder)
names(afinn_list) <- c('word', 'score')
afinn_list$word <- tolower(afinn_list$word)
myCorpus<- tm_map(clean_corpus, stemDocument)
library(tm)
myCorpus<- tm_map(clean_corpus, stemDocument)
# conver the corpus to a dataframe after stemming
dataframe<-data.frame(text = unlist(sapply(myCorpus, `[`, "content")), stringsAsFactors=F)
comments <- read.csv(file.choose(), header = T) #amex_comments.csv file
messages <- comments$message
messages <- gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
myCorpus <- Corpus(VectorSource(messages)) # create corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords())
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get", "amex", "card", "american", "express", "uub", "also", "just", "didnt", "till", "eue", "ufu", "since", "now", "let", "using", "hii","make", "even", "dont", "sure", "uu", "hi", "th", "oh", "uuucu", "ne", "ive", "ur", "ki", "ko", "uuub", "uubu"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
myCorpus<- tm_map(clean_corpus, stemDocument)
# conver the corpus to a dataframe after stemming
dataframe<-data.frame(text = unlist(sapply(myCorpus, `[`, "content")), stringsAsFactors=F)
install.packages("SnowballC")
library(SnowballC)
myCorpus<- tm_map(clean_corpus, stemDocument)
# conver the corpus to a dataframe after stemming
dataframe<-data.frame(text = unlist(sapply(myCorpus, `[`, "content")), stringsAsFactors=F)
dataframe$text <- as.character(dataframe$text)
useful_posts <- dataframe[!is.na(dataframe$text) & (dataframe$text !=""), ]
dataframe<-as.data.frame(useful_posts, stringsAsFactors=F)
dataframe$useful_posts <- as.character(dataframe$useful_posts)
useful_posts <- dataframe[!is.na(dataframe$useful_posts) & (dataframe$useful_posts !=" "), ]
# drop comments which are NA or empty
useful_comments <- dataframe[!is.na(dataframe$text) & (dataframe$text !=""), ]
messages <- comments$message
messages <- gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
myCorpus <- Corpus(VectorSource(messages)) # create corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords())
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get", "amex", "card", "american", "express", "uub", "also", "just", "didnt", "till", "eue", "ufu", "since", "now", "let", "using", "hii","make", "even", "dont", "sure", "uu", "hi", "th", "oh", "uuucu", "ne", "ive", "ur", "ki", "ko", "uuub", "uubu"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
library(SnowballC)
myCorpus<- tm_map(clean_corpus, stemDocument)
# conver the corpus to a dataframe after stemming
dataframe<-data.frame(text = unlist(sapply(myCorpus, `[`, "content")), stringsAsFactors=F)
dataframe$text <- as.character(dataframe$text)
useful_comments <- dataframe[!is.na(dataframe$text) & (dataframe$text !=""), ]
dataframe <- as.data.frame(useful_comments, stringsAsFactors = F)
dataframe$useful_comments <- as.character(dataframe$useful_comments)
useful_comments <- dataframe[!is.na(dataframe$useful_comments) & (dataframe$useful_comments !=" "), ]
head(useful_comments)
head(useful_comments)
messages <- comments$message
messages <- gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
myCorpus <- Corpus(VectorSource(messages)) # create corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords())
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get", "amex", "card", "american", "express", "uub", "also", "just", "didnt", "till", "eue", "ufu", "since", "now", "let", "using", "hii","make", "even", "dont", "sure", "uu", "hi", "th", "oh", "uuucu", "ne", "ive", "ur", "ki", "ko", "uuub", "uubu"))
# conver the corpus to a dataframe after stemming
# conver the corpus to a dataframe after stemming
dataframe<-data.frame(text = unlist(sapply(myCorpus, `[`, "content")), stringsAsFactors=F)
dataframe$text <- as.character(dataframe$text)
# drop comments which are NA or empty
useful_comments <- dataframe[!is.na(dataframe$text) & (dataframe$text !=""), ]
dataframe <- as.data.frame(useful_comments, stringsAsFactors = F)
dataframe$useful_comments <- as.character(dataframe$useful_comments)
useful_comments <- dataframe[!is.na(dataframe$useful_comments) & (dataframe$useful_comments !=" "), ]
head(useful_comments)
#load the AFINN-111 dictionary which contains English words and their positive/negative score
afinn_list <- read.delim(file.choose(), header = FALSE, stringsAsFactors = FALSE) # import AFINN-111.txt file (check data folder)
names(afinn_list) <- c('word', 'score')
afinn_list$word <- tolower(afinn_list$word)
vNegTerms <- afinn_list$word[afinn_list$score==-5 | afinn_list$score==-4]
negTerms <- c(afinn_list$word[afinn_list$score==-3 | afinn_list$score==-2 | afinn_list$score==-1], "second-rate", "moronic", "third-rate", "flawed", "juvenile", "boring", "distasteful", "ordinary", "disgusting", "senseless", "static", "brutal", "confused", "disappointing", "bloody", "silly", "tired", "predictable", "stupid", "uninteresting", "trite", "uneven", "outdated", "dreadful", "bland")
posTerms <- c(afinn_list$word[afinn_list$score==3 | afinn_list$score==2 | afinn_list$score==1], "first-rate", "insightful", "clever", "charming", "comical", "charismatic", "enjoyable", "absorbing", "sensitive", "intriguing", "powerful", "pleasant", "surprising", "thought-provoking", "imaginative", "unpretentious")
vPosTerms <- c(afinn_list$word[afinn_list$score==5 | afinn_list$score==4], "uproarious", "riveting", "fascinating", "dazzling", "legendary")
sentimentScore <- function(sentences, vNegTerms, negTerms, posTerms, vPosTerms){
final_scores <- matrix('', 0, 5)
# calculate the score for each word
scores <- laply(sentences, function(sentence, vNegTerms, negTerms, posTerms, vPosTerms){
#tokenize words
wordList <- str_split(sentence, '\\s+')
words <- unlist(wordList)
#build vector with matches between sentence and each category
vPosMatches <- match(words, vPosTerms)
posMatches <- match(words, posTerms)
vNegMatches <- match(words, vNegTerms)
negMatches <- match(words, negTerms)
#sum up number of words in each category
vPosMatches <- sum(!is.na(vPosMatches))
posMatches <- sum(!is.na(posMatches))
vNegMatches <- sum(!is.na(vNegMatches))
negMatches <- sum(!is.na(negMatches))
score <- c(vNegMatches, negMatches, posMatches, vPosMatches)
#add row to scores table
newrow <- c(initial_sentence, score)
final_scores <- rbind(final_scores, newrow)
return(final_scores)
}, vNegTerms, negTerms, posTerms, vPosTerms)
return(scores)
}
commentResult <- as.data.frame(sentimentScore(useful_comments, vNegTerms, negTerms, posTerms, vPosTerms))
sentimentScore <- function(sentences, vNegTerms, negTerms, posTerms, vPosTerms){
final_scores <- matrix('', 0, 5)
# calculate the score for each word
scores <- laply(sentences, function(sentence, vNegTerms, negTerms, posTerms, vPosTerms){
initial_sentence <- sentence
#tokenize words
wordList <- str_split(sentence, '\\s+')
words <- unlist(wordList)
#build vector with matches between sentence and each category
vPosMatches <- match(words, vPosTerms)
posMatches <- match(words, posTerms)
vNegMatches <- match(words, vNegTerms)
negMatches <- match(words, negTerms)
#sum up number of words in each category
vPosMatches <- sum(!is.na(vPosMatches))
posMatches <- sum(!is.na(posMatches))
vNegMatches <- sum(!is.na(vNegMatches))
negMatches <- sum(!is.na(negMatches))
score <- c(vNegMatches, negMatches, posMatches, vPosMatches)
#add row to scores table
newrow <- c(initial_sentence, score)
final_scores <- rbind(final_scores, newrow)
return(final_scores)
}, vNegTerms, negTerms, posTerms, vPosTerms)
return(scores)
}
commentResult <- as.data.frame(sentimentScore(useful_comments, vNegTerms, negTerms, posTerms, vPosTerms))
count_1 = sum(commentResult [,3])
count_1
head(commentResult)
count_1 = sum(commentResult[,2])
View(commentResult)
commentResult[,2]
commentResult$`2` = as.numeric(commentResult[,2])
count_1 = sum(commentResult[,2])
commentResult$`2` <- as.numeric(commentResult[,2])
commentResult$`3` <- as.numeric(commentResult[,3])
commentResult$`4` <- as.numeric(commentResult[,4])
commentResult$`5` <- as.numeric(commentResult[,5])
count_1 = sum(commentResult[,2])
count_2 = sum(commentResult[,3])
count_3 = sum(commentResult[,4])
count_4 = sum(commentResult[,5])
# bar plot for the sentiments for each category
count_sum = sum(count_1, count_2, count_3, count_4)
counts = c(count_1*100/count_sum,
count_2*100/count_sum,
count_3*100/count_sum,
count_4*100/count_sum)
names = c("VERY BAD","BAD","GOOD","VERY GOOD")
count_df = as.data.frame(counts, names)
gg2 <- ggplot(count_df, aes(x = names, y = counts, fill = names))
gg2 + geom_bar(stat = "identity", color = "black", alpha = 0.8, width = 0.5 ) +
labs(title = "Amex - Positive vs. Negative Sentiments") + guides(fill = FALSE) +
labs(x = "", y = "percentage") + theme_bw(base_size = 15) + scale_fill_manual(values = c("red","green", "red","green"))
library(ggplot2)
gg2 <- ggplot(count_df, aes(x = names, y = counts, fill = names))
gg2 + geom_bar(stat = "identity", color = "black", alpha = 0.8, width = 0.5 ) +
labs(title = "Amex - Positive vs. Negative Sentiments") + guides(fill = FALSE) +
labs(x = "", y = "percentage") + theme_bw(base_size = 15) + scale_fill_manual(values = c("red","green", "red","green"))
messages <- comments$message
messages <- gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
myCorpus <- Corpus(VectorSource(messages)) # create corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords())
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu", "uua", "udu", "uau","ucuuu", "uuuu", "uuu", "nnnn", "uuauub","uuauu","ueuu","can", "will", "get", "amex", "card", "american", "express", "uub", "also", "just", "didnt", "till", "eue", "ufu", "since", "now", "let", "using", "hii","make", "even", "dont", "sure", "uu", "hi", "th", "oh", "uuucu", "ne", "ive", "ur", "ki", "ko", "uuub", "uubu"))
# conver the corpus to a dataframe after stemming
dataframe<-data.frame(text = unlist(sapply(myCorpus, `[`, "content")), stringsAsFactors=F)
dataframe$text <- as.character(dataframe$text)
# drop comments which are NA or empty
useful_comments <- dataframe[!is.na(dataframe$text) & (dataframe$text !=""), ]
dataframe <- as.data.frame(useful_comments, stringsAsFactors = F)
dataframe$useful_comments <- as.character(dataframe$useful_comments)
useful_comments <- dataframe[!is.na(dataframe$useful_comments) & (dataframe$useful_comments !=" "), ]
head(useful_comments)
#load the AFINN-111 dictionary which contains English words and their positive/negative score
names(afinn_list) <- c('word', 'score')
afinn_list$word <- tolower(afinn_list$word)
vNegTerms <- afinn_list$word[afinn_list$score==-5 | afinn_list$score==-4]
negTerms <- c(afinn_list$word[afinn_list$score==-3 | afinn_list$score==-2 | afinn_list$score==-1], "second-rate", "moronic", "third-rate", "flawed", "juvenile", "boring", "distasteful", "ordinary", "disgusting", "senseless", "static", "brutal", "confused", "disappointing", "bloody", "silly", "tired", "predictable", "stupid", "uninteresting", "trite", "uneven", "outdated", "dreadful", "bland")
posTerms <- c(afinn_list$word[afinn_list$score==3 | afinn_list$score==2 | afinn_list$score==1], "first-rate", "insightful", "clever", "charming", "comical", "charismatic", "enjoyable", "absorbing", "sensitive", "intriguing", "powerful", "pleasant", "surprising", "thought-provoking", "imaginative", "unpretentious")
vPosTerms <- c(afinn_list$word[afinn_list$score==5 | afinn_list$score==4], "uproarious", "riveting", "fascinating", "dazzling", "legendary")
sentimentScore <- function(sentences, vNegTerms, negTerms, posTerms, vPosTerms){
final_scores <- matrix('', 0, 5)
# calculate the score for each word
scores <- laply(sentences, function(sentence, vNegTerms, negTerms, posTerms, vPosTerms){
initial_sentence <- sentence
#tokenize words
wordList <- str_split(sentence, '\\s+')
words <- unlist(wordList)
#build vector with matches between sentence and each category
vPosMatches <- match(words, vPosTerms)
posMatches <- match(words, posTerms)
vNegMatches <- match(words, vNegTerms)
negMatches <- match(words, negTerms)
#sum up number of words in each category
vPosMatches <- sum(!is.na(vPosMatches))
posMatches <- sum(!is.na(posMatches))
vNegMatches <- sum(!is.na(vNegMatches))
negMatches <- sum(!is.na(negMatches))
score <- c(vNegMatches, negMatches, posMatches, vPosMatches)
#add row to scores table
newrow <- c(initial_sentence, score)
final_scores <- rbind(final_scores, newrow)
return(final_scores)
}, vNegTerms, negTerms, posTerms, vPosTerms)
return(scores)
}
commentResult <- as.data.frame(sentimentScore(useful_comments, vNegTerms, negTerms, posTerms, vPosTerms))
commentResult$`2` <- as.numeric(commentResult[,2])
commentResult$`3` <- as.numeric(commentResult[,3])
commentResult$`4` <- as.numeric(commentResult[,4])
commentResult$`5` <- as.numeric(commentResult[,5])
count_1 = sum(commentResult[,2])
count_2 = sum(commentResult[,3])
count_3 = sum(commentResult[,4])
count_4 = sum(commentResult[,5])
# bar plot for the sentiments for each category
count_sum = sum(count_1, count_2, count_3, count_4)
counts = c(count_1*100/count_sum,
count_2*100/count_sum,
count_3*100/count_sum,
count_4*100/count_sum)
names = c("VERY BAD","BAD","GOOD","VERY GOOD")
count_df = as.data.frame(counts, names)
gg2 <- ggplot(count_df, aes(x = names, y = counts, fill = names))
gg2 + geom_bar(stat = "identity", color = "black", alpha = 0.8, width = 0.5 ) +
labs(title = "Positive vs. Negative Sentiments") + guides(fill = FALSE) +
labs(x = "", y = "% of comments") + theme_bw(base_size = 15) + scale_fill_manual(values = c("red","green", "red","green"))
