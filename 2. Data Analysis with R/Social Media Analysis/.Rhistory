score <- c(vNegMatches, negMatches, posMatches, vPosMatches)
#add row to scores table
newrow <- c(initial_sentence, score)
final_scores <- rbind(final_scores, newrow)
return(final_scores)
}, vNegTerms, negTerms, posTerms, vPosTerms)
return(scores)
}
commentResult <- as.data.frame(sentimentScore(useful_comments,
vNegTerms,
negTerms,
posTerms,
vPosTerms))
# count sentiment scores to plot
commentResult$`2` <- as.numeric(commentResult[,2])
commentResult$`3` <- as.numeric(commentResult[,3])
commentResult$`4` <- as.numeric(commentResult[,4])
commentResult$`5` <- as.numeric(commentResult[,5])
count_1 <- sum(commentResult[,2])
count_2 <- sum(commentResult[,3])
count_3 <- sum(commentResult[,4])
count_4 <- sum(commentResult[,5])
# bar plot for the sentiments for each category
count_sum = sum(count_1, count_2, count_3, count_4)
counts = c(count_1*100/count_sum,
count_2*100/count_sum,
count_3*100/count_sum,
count_4*100/count_sum)
names <- c("Very Bad","Bad","Good","Very Good")
count_df <- as.data.frame(counts, names)
gg2 <- ggplot(count_df, aes(x = names, y = counts, fill = names))
gg2 + geom_bar(stat = "identity", color = "black", alpha = 0.8, width = 0.5 ) +
labs(title = "Positive vs. Negative Sentiments") +
guides(fill = FALSE) +
labs(x = "", y = "% of comments") +
theme_bw(base_size = 15) +
scale_fill_manual(values = c("red","green", "red","green"))
library(ggplot2)
# count sentiment scores to plot
commentResult$`2` <- as.numeric(commentResult[,2])
commentResult$`3` <- as.numeric(commentResult[,3])
commentResult$`4` <- as.numeric(commentResult[,4])
commentResult$`5` <- as.numeric(commentResult[,5])
count_1 <- sum(commentResult[,2])
count_2 <- sum(commentResult[,3])
count_3 <- sum(commentResult[,4])
count_4 <- sum(commentResult[,5])
# bar plot for the sentiments for each category
count_sum = sum(count_1, count_2, count_3, count_4)
counts = c(count_1*100/count_sum,
count_2*100/count_sum,
count_3*100/count_sum,
count_4*100/count_sum)
names <- c("Very Bad","Bad","Good","Very Good")
count_df <- as.data.frame(counts, names)
gg2 <- ggplot(count_df, aes(x = names, y = counts, fill = names))
gg2 + geom_bar(stat = "identity", color = "black", alpha = 0.8, width = 0.5 ) +
labs(title = "Positive vs. Negative Sentiments") +
guides(fill = FALSE) +
labs(x = "", y = "% of comments") +
theme_bw(base_size = 15) +
scale_fill_manual(values = c("red","green", "red","green"))
colnames(comments)
library(slam)
library(tm)
library(stringr)
library(ggplot2)
comments <- read.csv(file.choose(), header = T) #amex_comments.csv file
messages <- comments$message
# remove non-alphanumeric characters first
messages <- gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3]
# Create a corpus
myCorpus <- Corpus(VectorSource(messages)) # create corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords())
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
# remove stop words and nonsensical words after examining the corpus
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu",
"uua", "udu", "uau","ucuuu", "uuuu", "uuu",
"nnnn", "uuauub","uuauu","ueuu","can", "will",
"get", "amex", "card", "american", "express",
"uub", "also", "just", "didnt", "till", "eue",
"ufu", "since", "now", "let", "using", "hii",
"make", "even", "dont", "sure", "uu", "hi",
"th", "oh", "uuucu", "ne", "ive", "ur", "ki",
"ko", "uuub", "uubu"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(2, 15)))
# convert the corpus to a data frame
dataframe<-data.frame(text = unlist(sapply(clean_corpus, `[`, "content")), stringsAsFactors=F)
dataframe$text <- as.character(dataframe$text)
dataframe["post_id"] <- NA
dataframe$post_id <- comments$post_id
# drop comments which are NA or empty
useful_comments <- dataframe[!is.na(dataframe$text) & (dataframe$text !=""), ]
dataframe <- as.data.frame(useful_comments, stringsAsFactors = F)
dataframe$useful_comments <- as.character(dataframe$useful_comments)
head(useful_comments)
dataframe$useful_comments <- as.character(dataframe$text)
useful_comments <- dataframe[!is.na(dataframe$useful_comments) & (dataframe$useful_comments !=" "), ]
sentimentScore <- function(sentences, vNegTerms, negTerms, posTerms, vPosTerms){
final_scores <- matrix('', 0, 5)
# calculate the score for each word
scores <- lapply(sentences, function(sentence, vNegTerms, negTerms, posTerms, vPosTerms){
initial_sentence <- sentence
#tokenize words
wordList <- str_split(sentence, '\\s+')
words <- unlist(wordList)
#build vector with matches between sentence and each category
vPosMatches <- match(words, vPosTerms)
posMatches <- match(words, posTerms)
vNegMatches <- match(words, vNegTerms)
negMatches <- match(words, negTerms)
#sum up number of words in each category
vPosMatches <- sum(!is.na(vPosMatches))
posMatches <- sum(!is.na(posMatches))
vNegMatches <- sum(!is.na(vNegMatches))
negMatches <- sum(!is.na(negMatches))
score <- c(vNegMatches, negMatches, posMatches, vPosMatches)
#add row to scores table
newrow <- c(initial_sentence, score)
final_scores <- rbind(final_scores, newrow)
return(final_scores)
}, vNegTerms, negTerms, posTerms, vPosTerms)
return(scores)
}
head(useful_comments)
library(slam)
library(tm)
library(stringr)
library(ggplot2)
comments <- read.csv(file.choose(), header = T) #amex_comments.csv file
messages <- comments$message
# remove non-alphanumeric characters first
messages <- gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3]
# Create a corpus
myCorpus <- Corpus(VectorSource(messages)) # create corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords())
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
# remove stop words and nonsensical words after examining the corpus
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu",
"uua", "udu", "uau","ucuuu", "uuuu", "uuu",
"nnnn", "uuauub","uuauu","ueuu","can", "will",
"get", "amex", "card", "american", "express",
"uub", "also", "just", "didnt", "till", "eue",
"ufu", "since", "now", "let", "using", "hii",
"make", "even", "dont", "sure", "uu", "hi",
"th", "oh", "uuucu", "ne", "ive", "ur", "ki",
"ko", "uuub", "uubu"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(2, 15)))
# convert the corpus to a data frame
dataframe<-data.frame(text = unlist(sapply(clean_corpus, `[`, "content")), stringsAsFactors=F)
dataframe$text <- as.character(dataframe$text)
dataframe["post_id"] <- NA
dataframe$post_id <- comments$post_id
# drop comments which are NA or empty
dim(dataframe)
useful_comments <- dataframe[!is.na(dataframe$text) & (dataframe$text !=""), ]
dim(useful_comments])
dim(useful_comments)
head(useful_comments)
View(useful_comments)
View(useful_comments)
dataframe <- as.data.frame(useful_comments, stringsAsFactors = F)
dim(dataframe)
library(slam)
library(tm)
library(stringr)
library(ggplot2)
comments <- read.csv(file.choose(), header = T) #amex_comments.csv file
messages <- comments$message
# remove non-alphanumeric characters first
messages <- gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3]
# Create a corpus
myCorpus <- Corpus(VectorSource(messages)) # create corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords())
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
# remove stop words and nonsensical words after examining the corpus
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu",
"uua", "udu", "uau","ucuuu", "uuuu", "uuu",
"nnnn", "uuauub","uuauu","ueuu","can", "will",
"get", "amex", "card", "american", "express",
"uub", "also", "just", "didnt", "till", "eue",
"ufu", "since", "now", "let", "using", "hii",
"make", "even", "dont", "sure", "uu", "hi",
"th", "oh", "uuucu", "ne", "ive", "ur", "ki",
"ko", "uuub", "uubu"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(2, 15)))
# convert the corpus to a data frame
dataframe<-data.frame(text = unlist(sapply(clean_corpus, `[`, "content")), stringsAsFactors=F)
dataframe$text <- as.character(dataframe$text)
dataframe["post_id"] <- NA
dataframe$post_id <- comments$post_id
# drop comments which are NA or empty
dim(dataframe)
useful_comments <- dataframe[!is.na(dataframe$text) &
(dataframe$text !="") &
(dataframe$text !=" "), ]
dim(useful_comments)
dim(dataframe)
head(useful_comments)
View(useful_comments)
sentimentScore <- function(sentences, vNegTerms, negTerms, posTerms, vPosTerms){
final_scores <- matrix('', 0, 5)
# calculate the score for each word
scores <- lapply(sentences, function(sentence, vNegTerms, negTerms, posTerms, vPosTerms){
initial_sentence <- sentence
#tokenize words
wordList <- str_split(sentence, '\\s+')
words <- unlist(wordList)
#build vector with matches between sentence and each category
vPosMatches <- match(words, vPosTerms)
posMatches <- match(words, posTerms)
vNegMatches <- match(words, vNegTerms)
negMatches <- match(words, negTerms)
#sum up number of words in each category
vPosMatches <- sum(!is.na(vPosMatches))
posMatches <- sum(!is.na(posMatches))
vNegMatches <- sum(!is.na(vNegMatches))
negMatches <- sum(!is.na(negMatches))
score <- c(vNegMatches, negMatches, posMatches, vPosMatches)
#add row to scores table
newrow <- c(initial_sentence, score)
final_scores <- rbind(final_scores, newrow)
return(final_scores)
}, vNegTerms, negTerms, posTerms, vPosTerms)
return(scores)
}
commentResult <- as.data.frame(sentimentScore(useful_comments$text,
vNegTerms,
negTerms,
posTerms,
vPosTerms))
# count sentiment scores to plot
commentResult$`2` <- as.numeric(commentResult[,2])
commentResult$`3` <- as.numeric(commentResult[,3])
commentResult$`4` <- as.numeric(commentResult[,4])
commentResult$`5` <- as.numeric(commentResult[,5])
count_1 <- sum(commentResult[,2])
count_2 <- sum(commentResult[,3])
count_3 <- sum(commentResult[,4])
count_4 <- sum(commentResult[,5])
dim(useful_comments)
commentResult
commentResult <- as.data.frame(sentimentScore(as.character(useful_comments$text),
vNegTerms,
negTerms,
posTerms,
vPosTerms))
commentResult$
# count sentiment scores to plot
commentResult$`2` <- as.numeric(commentResult[,2])
commentResult$`3` <- as.numeric(commentResult[,3])
commentResult$`4` <- as.numeric(commentResult[,4])
commentResult$`5` <- as.numeric(commentResult[,5])
count_1 <- sum(commentResult[,2])
commentResult$2
commentResult$X3.399
library(slam)
library(tm)
library(stringr)
library(ggplot2)
comments <- read.csv(file.choose(), header = T) #amex_comments.csv file
messages <- comments$message
# remove non-alphanumeric characters first
messages <- gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3]
# Create a corpus
myCorpus <- Corpus(VectorSource(messages)) # create corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords())
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
# remove stop words and nonsensical words after examining the corpus
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu",
"uua", "udu", "uau","ucuuu", "uuuu", "uuu",
"nnnn", "uuauub","uuauu","ueuu","can", "will",
"get", "amex", "card", "american", "express",
"uub", "also", "just", "didnt", "till", "eue",
"ufu", "since", "now", "let", "using", "hii",
"make", "even", "dont", "sure", "uu", "hi",
"th", "oh", "uuucu", "ne", "ive", "ur", "ki",
"ko", "uuub", "uubu"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(2, 15)))
# convert the corpus to a data frame
dataframe<-data.frame(text = unlist(sapply(clean_corpus, `[`, "content")), stringsAsFactors=F)
dataframe$text <- as.character(dataframe$text)
dataframe["post_id"] <- NA
dataframe$post_id <- comments$post_id
# drop comments which are NA or empty
dim(dataframe)
useful_comments <- dataframe[!is.na(dataframe$text) &
(dataframe$text !="") &
(dataframe$text !=" "), ]
dim(useful_comments)
#dataframe <- as.data.frame(useful_comments, stringsAsFactors = F)
dim(dataframe)
#dataframe$useful_comments <- as.character(dataframe$text)
#useful_comments <- dataframe[!is.na(dataframe$useful_comments) & (dataframe$useful_comments !=" "), ]
#head(useful_comments)
#View(useful_comments)
sentimentScore <- function(sentences, vNegTerms, negTerms, posTerms, vPosTerms){
final_scores <- matrix('', 0, 5)
# calculate the score for each word
scores <- lapply(sentences, function(sentence, vNegTerms, negTerms, posTerms, vPosTerms){
initial_sentence <- sentence
#tokenize words
wordList <- str_split(sentence, '\\s+')
words <- unlist(wordList)
#build vector with matches between sentence and each category
vPosMatches <- match(words, vPosTerms)
posMatches <- match(words, posTerms)
vNegMatches <- match(words, vNegTerms)
negMatches <- match(words, negTerms)
#sum up number of words in each category
vPosMatches <- sum(!is.na(vPosMatches))
posMatches <- sum(!is.na(posMatches))
vNegMatches <- sum(!is.na(vNegMatches))
negMatches <- sum(!is.na(negMatches))
score <- c(vNegMatches, negMatches, posMatches, vPosMatches)
#add row to scores table
newrow <- c(initial_sentence, score)
final_scores <- rbind(final_scores, newrow)
return(final_scores)
}, vNegTerms, negTerms, posTerms, vPosTerms)
return(scores)
}
commentResult <- as.data.frame(sentimentScore(as.character(useful_comments$text),
vNegTerms,
negTerms,
posTerms,
vPosTerms))
commentResult$
commentResult$`2` <- as.numeric(commentResult[,2])
commentResult$`3` <- as.numeric(commentResult[,3])
commentResult$`4` <- as.numeric(commentResult[,4])
commentResult$`5` <- as.numeric(commentResult[,5])
count_1 <- sum(commentResult[,2])
dataframe <- as.data.frame(useful_comments, stringsAsFactors = F)
dim(dataframe)
dim(useful_comments)
dataframe$useful_comments <- as.character(dataframe$text)
useful_comments <- dataframe[!is.na(dataframe$useful_comments) & (dataframe$useful_comments !=" "), ]
head(useful_comments)
commentResult <- as.data.frame(sentimentScore(as.character(useful_comments$useful_comments),
vNegTerms,
negTerms,
posTerms,
vPosTerms))
commentResult$`2` <- as.numeric(commentResult[,2])
commentResult$`3` <- as.numeric(commentResult[,3])
commentResult$`4` <- as.numeric(commentResult[,4])
commentResult$`5` <- as.numeric(commentResult[,5])
count_1 <- sum(commentResult[,2])
count_2 <- sum(commentResult[,3])
count_3 <- sum(commentResult[,4])
count_4 <- sum(commentResult[,5])
library(slam)
library(tm)
library(stringr)
library(ggplot2)
comments <- read.csv(file.choose(), header = T) #amex_comments.csv file
comments <- read.csv(file.choose(), header = T) #amex_comments.csv file
messages <- comments$message
# remove non-alphanumeric characters first
messages <- gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", messages)))
messages[1:3]
# Create a corpus
myCorpus <- Corpus(VectorSource(messages)) # create corpus
# convert to lower case
clean_corpus <- tm_map(myCorpus, PlainTextDocument)
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))
# remove numbers from the corpus
clean_corpus <- tm_map(clean_corpus, removeNumbers)
# remove punctuations if present
clean_corpus <- tm_map(clean_corpus, removePunctuation)
# remove stop words
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords())
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
# remove stop words and nonsensical words after examining the corpus
# also removing obvious high-frequency words such as amex, card, american, express, etc.
clean_corpus <- tm_map(clean_corpus, removeWords, c(stopwords('english'), "uue", "uuue", "uuuu",
"uua", "udu", "uau","ucuuu", "uuuu", "uuu",
"nnnn", "uuauub","uuauu","ueuu","can", "will",
"get", "amex", "card", "american", "express",
"uub", "also", "just", "didnt", "till", "eue",
"ufu", "since", "now", "let", "using", "hii",
"make", "even", "dont", "sure", "uu", "hi",
"th", "oh", "uuucu", "ne", "ive", "ur", "ki",
"ko", "uuub", "uubu"))
# strip the extra whitespace
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
tdm <- TermDocumentMatrix(clean_corpus, control = list(wordLengths = c(2, 15)))
# convert the corpus to a data frame
dataframe<-data.frame(text = unlist(sapply(clean_corpus, `[`, "content")), stringsAsFactors=F)
dataframe$text <- as.character(dataframe$text)
dataframe["post_id"] <- NA
dataframe$post_id <- comments$post_id
# drop comments which are NA or empty
dim(dataframe)
useful_comments <- dataframe[!is.na(dataframe$text) &
(dataframe$text !="") &
(dataframe$text !=" "), ]
dim(useful_comments)
dataframe <- as.data.frame(useful_comments, stringsAsFactors = F)
dim(dataframe)
dataframe$useful_comments <- as.character(dataframe$text)
useful_comments <- dataframe[!is.na(dataframe$useful_comments) & (dataframe$useful_comments !=" "), ]
head(useful_comments)
#View(useful_comments)
#load the AFINN-111 lexicon which contains English words and their positive/negative score
afinn_list <- read.delim(file.choose(), header = FALSE, stringsAsFactors = FALSE) # import AFINN-111.txt file (check data folder)
names(afinn_list) <- c('word', 'score')
afinn_list$word <- tolower(afinn_list$word)
vNegTerms <- afinn_list$word[afinn_list$score == -5 | afinn_list$score == -4]
negTerms <- c(afinn_list$word[afinn_list$score == -3 | afinn_list$score == -2 | afinn_list$score == -1],
"second-rate", "moronic", "third-rate", "flawed", "juvenile", "boring", "distasteful",
"ordinary", "disgusting", "senseless", "static", "brutal", "confused", "disappointing",
"bloody", "silly", "tired", "predictable", "stupid", "uninteresting", "trite", "uneven",
"outdated", "dreadful", "bland")
posTerms <- c(afinn_list$word[afinn_list$score == 3 | afinn_list$score == 2 | afinn_list$score == 1],
"first-rate", "insightful", "clever", "charming", "comical", "charismatic",
"enjoyable", "absorbing", "sensitive", "intriguing", "powerful", "pleasant",
"surprising", "thought-provoking", "imaginative", "unpretentious")
vPosTerms <- c(afinn_list$word[afinn_list$score == 5 | afinn_list$score == 4],
"uproarious", "riveting", "fascinating", "dazzling", "legendary")
sentimentScore <- function(sentences, vNegTerms, negTerms, posTerms, vPosTerms){
final_scores <- matrix('', 0, 5)
# calculate the score for each word
scores <- lapply(sentences, function(sentence, vNegTerms, negTerms, posTerms, vPosTerms){
initial_sentence <- sentence
#tokenize words
wordList <- str_split(sentence, '\\s+')
words <- unlist(wordList)
#build vector with matches between sentence and each category
vPosMatches <- match(words, vPosTerms)
posMatches <- match(words, posTerms)
vNegMatches <- match(words, vNegTerms)
negMatches <- match(words, negTerms)
#sum up number of words in each category
vPosMatches <- sum(!is.na(vPosMatches))
posMatches <- sum(!is.na(posMatches))
vNegMatches <- sum(!is.na(vNegMatches))
negMatches <- sum(!is.na(negMatches))
score <- c(vNegMatches, negMatches, posMatches, vPosMatches)
#add row to scores table
newrow <- c(initial_sentence, score)
final_scores <- rbind(final_scores, newrow)
return(final_scores)
}, vNegTerms, negTerms, posTerms, vPosTerms)
return(scores)
}
commentResult <- as.data.frame(sentimentScore(as.character(useful_comments$useful_comments),
vNegTerms,
negTerms,
posTerms,
vPosTerms))
# count sentiment scores to plot
commentResult$`2` <- as.numeric(commentResult[,2])
commentResult$`3` <- as.numeric(commentResult[,3])
commentResult$`4` <- as.numeric(commentResult[,4])
commentResult$`5` <- as.numeric(commentResult[,5])
count_1 <- sum(commentResult[,2])
count_2 <- sum(commentResult[,3])
