---
title: "Random Forest"
author: "Riddhik Rathod | @DataAstrologer"
date: "April 17, 2016"
output: html_document
---

# Random Forest

* The code below demonstrates Random Forest.
* Random Forest is a part of supervised machine learning and thus the dataset is split into **training and testing**.
* The Random Forest algorithm uses **random ensemble decision trees** for classification.
* The **credit.csv** dataset from the data folder is used here. This dataset represent loans obtained from a credit agency in Germany. The currency is recorded in Deutsche Marks (DM). The dataset contains various attributes for predicting whether a given customer would default a loan or not based on various factor such as balance, age, etc. The **dependent variable (default)** can have a value of either 1 (Non-Default) or 2 (Default).

## Implementation in R

The credit.csv dataset is present in the data folder.
```{r, eval=TRUE}
credit = read.csv("./data/credit.csv", header = T)
```

Exploratory data analysis of the variable types.
```{r, eval=TRUE, include=TRUE}
str(credit)
```
![str_credit](./data/images/str_credit.png)

Converting the default variable to factor.
```{r, eval=TRUE, include=TRUE}
credit$default = as.factor(credit$default)
```

Summary of the features of the dataset.
```{r}
summary(credit)
```
![summary_credit](./data/images/summary_credit.png)

Scatter plot matrix to visualize numeric data.
```{r}
plot(credit[, c(2, 5, 8, 11, 13, 16, 18)])
```
![plot_credit](./data/images/plot_credit.png)

Let's take a look at some of the ```table()``` outputs for a couple of features of loans that seem likely to predict a default. The checking_balance and savings_balance features indicate the applicant's checking and savings account balance, and are recorded as categorical variables.
```{r}
table(credit$checking_balance)
```
![credit_checking](./data/images/credit_checking.png)

```{r}
table(credit$savings_balance)
```
![credit_saving](./data/images/credit_saving.png)

### Splitting the dataset

The dataset is split into two parts: *training* and *testing*. The training part is used for fitting the model and the testing part is used for assessing the model. The split is done randomly to eliminate bias. The ```sample()``` function in R is used for generating 800 random samples as training data and the remaining as testing data. 
```{r}
set.seed(100) # to control randomness and get similar results

train = sample(1:1000, 800)
test = -train

training_data = credit[train, ]
testing_data = credit[test, ]
```

### Random Forest Model

The ```randomForest()``` function from the ```randomForest``` package is used for fitting the model to the dataset. The argument ```mtry = 6``` indicates that 6 predictors should be considered for each split of the tree. By default, ```randomForest()``` uses sqrt(p) variables when building a random forest classification tree (p = predictors).
```{r}
#install.packages("randomForest")
library(randomForest)

model = randomForest(default ~ ., data = training_data, mtry = 6, importance = TRUE)
model
```
![rf_model_2](./data/images/rf_model_2.png)

The model command above shows that the number of trees constructed were 500. Also, the Out-of-Bag error rate is 25.12%. 

The plot below shows the misclassification rate of the model when constructing the random forest. As the forest is built on the training data, each tree is tested on the 1/3rd of the samples (36.8%) not used in building that tree. This internal error estimate, known as the **Out-of-Bag error**, is plotted against the number of trees. Errors related to the 1 (Non-Default) and 2 (Default) are also plotted.
```{r}
layout(matrix(c(1, 2), nrow = 1), width = c(4, 1))
par(mar = c(5, 4, 4, 0)) #No margin on the right side
plot(model, main = "Random Forest Model")
par(mar = c(5, 0, 4, 2)) #No margin on the left side
plot(c(0, 1), type = "n", axes = F, xlab = "", ylab = "")
legend("top", colnames(model$err.rate), col = 1:3, cex = 0.8, fill = 1:3)
```
![rf_error_2](./data/images/rf_error_2.png)

The ```importance()``` function shows the importance of each variables in constructing the random forest. The important variables are listed below from the most important to the least. The values next to the important variables show the percentage of MSE increased if the variable is removed.
```{r}
imp = importance(model)[,1]
sort(imp, decreasing = TRUE)
```
![imp_model_2](.data/images/imp_model_2.png)

Each variable is plotted according to its importance below.
```{r}
barplot(sort(imp), 
        col = "red", 
        main = "Variable Importance Plot", 
        ylim = c(0,20),
        ylab = "% Increase MSE if Variable is Removed")
```
![rf_imp_2](./data/images/rf_imp_2.png)

Summary statistics for the first tree (k = 1) can be found by using the code below.
```{r}
summary(getTree(model, k = 1, labelVar = TRUE))
```
![rf_summary_1_2](./data/images/rf_summary_1_2.png)

### Prediction and Accuracy

In order to do predictions using the random forest model on the testing data, we use the ```predict()``` function in R.
```{r}
predicted_y = predict(model, testing_data)
```

A confusion matrix is used for checking the accuracy of the model. The ```confusionMatrix()``` function is a part of the ```caret``` package. It shows the true positives, false positives, true negatives and false negatives and hence the misclassification rate. 
```{r}
#install.packages("caret")
library(caret)
confusionMatrix(testing_data$default, predicted_y)
```
![cfm_rf_2](./data/images/cfm_rf_2.png)

### ROC Curve

The ROC curve is plotted using the ```performance()``` and the ```prediction()``` function present in the ```ROCR``` library. The R code to plot the curve is shown below.
```{r}
#install.packages("ROCR")
library(ROCR)

OOB.votes = predict(model, testing_data, type = "prob")
OOB.pred = OOB.votes[,2]

pred.obj = prediction(OOB.pred, testing_data$default)

ROC.perf = performance(pred.obj, "tpr", "fpr")
plot (ROC.perf, main = "True Positive vs False Positive for Random Forest Model")
```
![tpr_fpr](./data/images/tpr_fpr.png)

```{r}
RP.perf <- performance(pred.obj, "rec","prec")
plot (RP.perf, main = "Precision vs Recall Curve for Random Forest Model")
```
![rec_prec](./data/images/rec_prec.png)


## Bagging Model

The argument ```mtry = 13``` below indicates that all 13 predictors should be considered for each split of the tree. This is a special case of random forest known as **bagging** since all the predictor variables are included.  
```{r}
#install.packages("randomForest")
library(randomForest)

model = randomForest(default ~., data = training_data, mtry = 13, importance = TRUE)
model
```
![rf_model](./data/images/rf_model.png)

The model command above shows that the number of trees constructed were 500. Also, the Out-of-Bag error rate is 24.75%. 

The plot below shows the misclassification rate of the model when constructing the random forest. As the forest is built on the training data, each tree is tested on the 1/3rd of the samples (36.8%) not used in building that tree. This internal error estimate, known as the **Out-of-Bag error**, is plotted against the number of trees. Errors related to the 1 (Non-Default) and 2 (Default) are also plotted.
```{r}
layout(matrix(c(1, 2), nrow = 1), width = c(4, 1))
par(mar = c(5, 4, 4, 0)) #No margin on the right side
plot(model, main = "Bagging Model")
par(mar = c(5, 0, 4, 2)) #No margin on the left side
plot(c(0, 1), type = "n", axes = F, xlab = "", ylab = "")
legend("top", colnames(model$err.rate), col = 1:3, cex = 0.8, fill = 1:3)
```
![rf_error](./data/images/rf_error.png)

Using the ```importance()``` function, we can view the importance of each variable:
```{r}
imp = importance(model)[,1]
sort(imp, decreasing = TRUE)
```
![imp_model_1](.data/images/imp_model_1.png)

Each variable is plotted according to its importance below.
```{r}
barplot(sort(imp), 
        col = "blue", 
        main = "Variable Importance Plot", 
        ylim = c(0,20),
        ylab = "% Increase MSE if Variable is Removed")
```
![rf_imp](./data/images/rf_imp.png)

Summary statistics for the first tree (k = 1).
```{r}
summary(getTree(model, k = 1, labelVar = TRUE))
```
![rf_summary_1](./data/images/rf_summary_1.png)

In order to do predictions using the random forest model on the testing data, we use the ```predict()``` function in R.
```{r}
predicted_y = predict(model, testing_data)
```

A confusion matrix is used for checking the accuracy of the model. The ```confusionMatrix()``` function is a part of the ```caret``` package. It shows the true positives, false positives, true negatives and false negatives and hence the misclassification rate. 
```{r}
confusionMatrix(testing_data$default, predicted_y)
```
![confusion_matrix_bagging](./data/images/cfm_bagging.png)