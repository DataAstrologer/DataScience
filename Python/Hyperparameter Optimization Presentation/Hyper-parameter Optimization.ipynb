{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tuning the Algorithm Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tuning the model like tuning a guitar](how-to-tune-a-guitar.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below code can be split in secions as :\n",
    "\n",
    "* Import required datasets\n",
    "* User defined function to extract top \"n\" model\n",
    "* A small introduction to the data set used (self read)\n",
    "* Import dataset for processing\n",
    "* Grid Search\n",
    "    - create a grid dictionary of hyper-parameters\n",
    "    - execute the Grid Search Optimisation\n",
    "    - output the results\n",
    "* Random Search\n",
    "    - create a dictionary of hyper-parameters\n",
    "    - execute the Random Search Optimisation\n",
    "    - output the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The coding begins.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Import the required modules\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Digit Dataset\n",
    "\n",
    "This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, weâ€™d have to first transform it into a feature vector with length 64.\n",
    "\n",
    "![Digit](plot_digits_last_image_001.png)\n",
    "\n",
    "\n",
    "The info about the \"Digits\" dataset can be read from:\n",
    "* http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits\n",
    "\n",
    "* http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: -c: line 0: syntax error near unexpected token `plot_digits_last_image_001.png'\r\n",
      "/bin/sh: -c: line 0: `[Digit](plot_digits_last_image_001.png)'\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get some data\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 43.57 seconds for 216 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.934 (std: 0.012)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 3, 'min_samples_split': 1, 'criterion': 'gini', 'max_features': 10, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.933 (std: 0.011)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': 10, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.931 (std: 0.012)\n",
      "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 10, 'max_depth': None}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over all parameters\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [5, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(1, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 108\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 25.30 seconds for 108 candidates parameter settings.\n",
      "\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.935 (std: 0.003)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 4, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.932 (std: 0.011)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 1, 'criterion': 'gini', 'max_features': 9, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.929 (std: 0.005)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 5, 'criterion': 'gini', 'max_features': 10, 'max_depth': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "print(\"\\n\")\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison](GridRandomComparison2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            For half the number of iterations, Random Search gives almost the same Validation score as Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####  Light-reading stuff :-)  \n",
    "\n",
    "This is blog is about handwriting recognition and Python, the blog explains on how images are recognised, what training methods are used to recoginse and predict the digit from the image. \n",
    "\n",
    "http://blog.yhat.com/posts/digit-recognition-with-node-and-python.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "* Random Forest - http://goo.gl/F14BqE\n",
    "\n",
    "* GridSearchCV - http://goo.gl/Fca3kX\n",
    "\n",
    "* RandomizedSearchCV - http://goo.gl/T4MZct\n",
    "\n",
    "* How to find the best model parameters in scikit-learn (video) - https://goo.gl/1xDhtm\n",
    "\n",
    "* Random Search for Hyper-Parameter Optimization -  http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf\n",
    "\n",
    "* How to Evaluate Machine Learning Models: Hyperparameter Tuning by Alice Zheng  - http://goo.gl/B7KNJs\n",
    "\n",
    "* Comparing randomized search and grid search for hyperparameter estimation - http://goo.gl/9q1qgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Thank You](2ThankYou.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
