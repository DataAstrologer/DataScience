### LOGISTIC REGRESSION ###
puffinbill <- read.csv(file.choose(), header = T)
head(puffinbill)
str(puffinbill)
# Creating New Variables
sex <- puffinbill$sex
curlen <- puffinbill$curlen
# Coding Female as 1 and Male as 0
sexcode <- ifelse(sex == "F", 1, 0)
# Plotting the Data
plot(curlen, jitter(x = sexcode, factor = 0.15), pch = 19,
xlab = "Curvature Length", ylab = "Sex (0 - Male, 1 - Female)")
# Logistic Regression Model
model <- glm(sexcode ~ curlen, binomial)
model
summary(model)
# Creating a sequence from minimum to maximum Curvature Length in steps of 0.01
xv <- seq(min(curlen), max(curlen), 0.01)
# Predicting the values for the given xv values
ypredict <- predict(model, list(curlen = xv), type = "response")
lines(xv, ypredict, col ="red")
# Some other plots
library(popbio)
install.packages("popbio")
library(popbio)
logi.hist.plot(curlen, sexcode, boxp = F, type = "count", col = "gray",
xlabel = "size")
breaks <- seq(round(min(curlen)-1, digits = 0),
round(max(curlen)+1, digits = 0),
by = 1)
breaks
sexMale <- subset(x = puffinbill, subset = puffinbill$sex == "M")
sexFemale <- subset(x = puffinbill, subset = puffinbill$sex == "F")
sexMale.cut <- cut(sexMale$curlen, breaks, right = FALSE)
sexFemale.cut <- cut(sexFemale$curlen, breaks, right = FALSE)
sexMale.freq <- table(sexMale.cut)
sexMale.freq
sexFemale.freq <- table(sexFemale.cut)
sexFemale.freq
logi.hist.plot(curlen, sexcode, boxp = T, type = "count", col = "gray",
xlabel = "size")
logi.hist.plot(curlen, sexcode, boxp = F, type = "count", col = "blue",
xlabel = "size")
logi.hist.plot(sexcode, boxp = F, type = "count", col = "blue",
xlabel = "size")
plot(curlen, jitter(x = sexcode, factor = 0.15), pch = 19,
xlab = "Curvature Length", ylab = "Sex (0 - Male, 1 - Female)")
# Load the ISLR Package
library(ISLR)
?Smarket
## we would like to predict the direction of the stock price based on the predictor variables.
data <- Smarket
## Split the data into training and testing data
## since the data is time series, then it does not make sense to randomly split the data.
## instead we will use 2005 as our testing data and rest as our training data.
data
model1 = glm(Direction ~ Year+Lag1+Lag2+Volume, data = training_data, family = 'binomial')
summary(model1)
data <- Smarket
## Split the data into training and testing data
## since the data is time series, then it does not make sense to randomly split the data.
## instead we will use 2005 as our testing data and rest as our training data.
train = data$Year < 2005
test = !train
training_data = data[train,-8]
testing_data = data[test,-8]
## Train the model using training data from 2001 until 2005(excluded)
model = glm(Direction ~ .,
data = training_data,
family = "binomial")
summary(model)
## The only significant variable is the Year. The other lag variables are not significant,
## but Lag1, Lag2, and volume seems to be okay to keep in our model (even though they are not
## statistically significant) and this is because we are interested in predictions and not interpretation
## Train the model usig only these variables
model1 = glm(Direction ~ Year+Lag1+Lag2+Volume, data = training_data, family = 'binomial')
summary(model1)
### Visualize the coefficients to see how far they are from zero.
install.packages("coefplot")
library(coefplot)
coefplot(model1)
## zoom into the estimates of the variables without the intercept
coefplot(model, coefficients = c('Year','Lag1','Lag2','Lag3','Lag4','Lag5','Volume'))
coefplot(model1, coefficients = c('Year','Lag1','Lag2','Volume'))
summary(model1)
carseats = read.csv(file.choose(), header = TRUE)
# Emploratory data analysis
str(carseats)
summary(carseats)
# Converting categorical variables to factor
carseats$Sales = as.factor(carseats$Sales)
carseats$ShelveLoc = as.factor(carseats$ShelveLoc)
carseats$Urban = as.factor(carseats$Urban)
carseats$US = as.factor(carseats$US)
str(carseats)
summary(carseats)
plot(carseats)
str(carseats)
summary(carseats[,c(-1,-7,-10,-11)])
plot(carseats[,c(-1,-7,-10,-11)])
str(carseats)
data = read.csv(file.choose(), header = T)
set.seed(100) # to control randomness and get similar results
train = sample(1:400, 300)
test = -train
training_data = carsales[train,]
testing_data = carsales[test,]
carsales = read.csv(file.choose(), header = T)
set.seed(100) # to control randomness and get similar results
train = sample(1:400, 300)
test = -train
training_data = carsales[train,]
testing_data = carsales[test,]
model = glm(Sales ~.,data = training_data, family = binomial(link="logit"))
model
library(car)
vif(model)
corr_matrix = round(cor(training_data[,c(-1,-7,-10,-11)]),2)
corr_matrix
library(corrplot)
corrplot(corr_matrix, order = "hclust")
model.log = glm(Sales ~ CompPrice + Income + Advertising + Price + ShelveLoc + Age,
data = training_data,
family = binomial(link = "logit"))
summary(model.log)
logistic_probs = predict(model_2, testing_data, type = "response")
logistic_probs = predict(model.log, testing_data, type = "response")
for(i in 1:100){
if (logistic_probs[i] >= 0.500){
logistic_probs[i] = 1
}
if (logistic_probs[i] < 0.500){
logistic_probs[i] = 0
}
}
library(caret)
confusionMatrix(testing_data$Sales, logistic_probs)
#install.packages("caret")
data <- read.csv(file.choose(),header = T)
#Predictor Variables
Age <- data$Age
# REsponse Variable
choice <- data$Choice
#Regression Model
model_1 <- multinom(choice ~ Age, data = data)
library(nnet)
model_1 <- multinom(choice ~ Age, data = data)
library(nnet)
# Read the data from CSV
data <- read.csv(file.choose(),header = T)
#Predictor Variables
Age <- data$Age
# REsponse Variable
choice <- data$Choice
#Regression Model
model_1 <- multinom(choice ~ Age, data = data)
summary(model_1)
# Logistic REgression Example 2
# Read the data from CSV
data2 <- read.csv(file.choose(),header = T)
str(data2)
#Regression Model
model_2 <- multinom(data2$Loan.approval ~ Age+Salary, data = data2)
summary(model_2)
data = read.csv(file.choose(), header = TRUE)
set.seed(100) # to control randomness and get similar results
train = sample(1:23, 15)
test = -train
training_data = data[train,]
testing_data = data[test,]
library(nnet)
model_2 = multinom(training_data$Loan.approval ~ Age+Salary, data = data)
set.seed(100) # to control randomness and get similar results
train = sample(1:23, 15)
test = -train
training_data = data[train,]
testing_data = data[test,]
library(nnet)
model_2 = multinom(Loan.approval ~ Age+Salary, data = training_data)
lp = predict(model_2, testing_data)
library(caret)
confusionMatrix(testing_data$Loan.approval, lp)
