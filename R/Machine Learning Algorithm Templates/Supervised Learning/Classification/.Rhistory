### LOGISTIC REGRESSION ###
puffinbill <- read.csv(file.choose(), header = T)
head(puffinbill)
str(puffinbill)
# Creating New Variables
sex <- puffinbill$sex
curlen <- puffinbill$curlen
# Coding Female as 1 and Male as 0
sexcode <- ifelse(sex == "F", 1, 0)
# Plotting the Data
plot(curlen, jitter(x = sexcode, factor = 0.15), pch = 19,
xlab = "Curvature Length", ylab = "Sex (0 - Male, 1 - Female)")
# Logistic Regression Model
model <- glm(sexcode ~ curlen, binomial)
model
summary(model)
# Creating a sequence from minimum to maximum Curvature Length in steps of 0.01
xv <- seq(min(curlen), max(curlen), 0.01)
# Predicting the values for the given xv values
ypredict <- predict(model, list(curlen = xv), type = "response")
lines(xv, ypredict, col ="red")
# Some other plots
library(popbio)
install.packages("popbio")
library(popbio)
logi.hist.plot(curlen, sexcode, boxp = F, type = "count", col = "gray",
xlabel = "size")
breaks <- seq(round(min(curlen)-1, digits = 0),
round(max(curlen)+1, digits = 0),
by = 1)
breaks
sexMale <- subset(x = puffinbill, subset = puffinbill$sex == "M")
sexFemale <- subset(x = puffinbill, subset = puffinbill$sex == "F")
sexMale.cut <- cut(sexMale$curlen, breaks, right = FALSE)
sexFemale.cut <- cut(sexFemale$curlen, breaks, right = FALSE)
sexMale.freq <- table(sexMale.cut)
sexMale.freq
sexFemale.freq <- table(sexFemale.cut)
sexFemale.freq
logi.hist.plot(curlen, sexcode, boxp = T, type = "count", col = "gray",
xlabel = "size")
logi.hist.plot(curlen, sexcode, boxp = F, type = "count", col = "blue",
xlabel = "size")
logi.hist.plot(sexcode, boxp = F, type = "count", col = "blue",
xlabel = "size")
plot(curlen, jitter(x = sexcode, factor = 0.15), pch = 19,
xlab = "Curvature Length", ylab = "Sex (0 - Male, 1 - Female)")
# Load the ISLR Package
library(ISLR)
?Smarket
## we would like to predict the direction of the stock price based on the predictor variables.
data <- Smarket
## Split the data into training and testing data
## since the data is time series, then it does not make sense to randomly split the data.
## instead we will use 2005 as our testing data and rest as our training data.
data
model1 = glm(Direction ~ Year+Lag1+Lag2+Volume, data = training_data, family = 'binomial')
summary(model1)
data <- Smarket
## Split the data into training and testing data
## since the data is time series, then it does not make sense to randomly split the data.
## instead we will use 2005 as our testing data and rest as our training data.
train = data$Year < 2005
test = !train
training_data = data[train,-8]
testing_data = data[test,-8]
## Train the model using training data from 2001 until 2005(excluded)
model = glm(Direction ~ .,
data = training_data,
family = "binomial")
summary(model)
## The only significant variable is the Year. The other lag variables are not significant,
## but Lag1, Lag2, and volume seems to be okay to keep in our model (even though they are not
## statistically significant) and this is because we are interested in predictions and not interpretation
## Train the model usig only these variables
model1 = glm(Direction ~ Year+Lag1+Lag2+Volume, data = training_data, family = 'binomial')
summary(model1)
### Visualize the coefficients to see how far they are from zero.
install.packages("coefplot")
library(coefplot)
coefplot(model1)
## zoom into the estimates of the variables without the intercept
coefplot(model, coefficients = c('Year','Lag1','Lag2','Lag3','Lag4','Lag5','Volume'))
coefplot(model1, coefficients = c('Year','Lag1','Lag2','Volume'))
summary(model1)
carseats = read.csv(file.choose(), header = TRUE)
# Emploratory data analysis
str(carseats)
summary(carseats)
# Converting categorical variables to factor
carseats$Sales = as.factor(carseats$Sales)
carseats$ShelveLoc = as.factor(carseats$ShelveLoc)
carseats$Urban = as.factor(carseats$Urban)
carseats$US = as.factor(carseats$US)
str(carseats)
summary(carseats)
plot(carseats)
str(carseats)
summary(carseats[,c(-1,-7,-10,-11)])
plot(carseats[,c(-1,-7,-10,-11)])
str(carseats)
data = read.csv(file.choose(), header = T)
set.seed(100) # to control randomness and get similar results
train = sample(1:400, 300)
test = -train
training_data = carsales[train,]
testing_data = carsales[test,]
carsales = read.csv(file.choose(), header = T)
set.seed(100) # to control randomness and get similar results
train = sample(1:400, 300)
test = -train
training_data = carsales[train,]
testing_data = carsales[test,]
model = glm(Sales ~.,data = training_data, family = binomial(link="logit"))
model
library(car)
vif(model)
corr_matrix = round(cor(training_data[,c(-1,-7,-10,-11)]),2)
corr_matrix
library(corrplot)
corrplot(corr_matrix, order = "hclust")
model.log = glm(Sales ~ CompPrice + Income + Advertising + Price + ShelveLoc + Age,
data = training_data,
family = binomial(link = "logit"))
summary(model.log)
logistic_probs = predict(model_2, testing_data, type = "response")
logistic_probs = predict(model.log, testing_data, type = "response")
for(i in 1:100){
if (logistic_probs[i] >= 0.500){
logistic_probs[i] = 1
}
if (logistic_probs[i] < 0.500){
logistic_probs[i] = 0
}
}
library(caret)
confusionMatrix(testing_data$Sales, logistic_probs)
#install.packages("caret")
data <- read.csv(file.choose(),header = T)
#Predictor Variables
Age <- data$Age
# REsponse Variable
choice <- data$Choice
#Regression Model
model_1 <- multinom(choice ~ Age, data = data)
library(nnet)
model_1 <- multinom(choice ~ Age, data = data)
library(nnet)
# Read the data from CSV
data <- read.csv(file.choose(),header = T)
#Predictor Variables
Age <- data$Age
# REsponse Variable
choice <- data$Choice
#Regression Model
model_1 <- multinom(choice ~ Age, data = data)
summary(model_1)
# Logistic REgression Example 2
# Read the data from CSV
data2 <- read.csv(file.choose(),header = T)
str(data2)
#Regression Model
model_2 <- multinom(data2$Loan.approval ~ Age+Salary, data = data2)
summary(model_2)
data = read.csv(file.choose(), header = TRUE)
set.seed(100) # to control randomness and get similar results
train = sample(1:23, 15)
test = -train
training_data = data[train,]
testing_data = data[test,]
library(nnet)
model_2 = multinom(training_data$Loan.approval ~ Age+Salary, data = data)
set.seed(100) # to control randomness and get similar results
train = sample(1:23, 15)
test = -train
training_data = data[train,]
testing_data = data[test,]
library(nnet)
model_2 = multinom(Loan.approval ~ Age+Salary, data = training_data)
lp = predict(model_2, testing_data)
library(caret)
confusionMatrix(testing_data$Loan.approval, lp)
### LOGISTIC REGRESSION ###
puffinbill <- read.csv(file.choose(), header = T)
head(puffinbill)
str(puffinbill)
# Creating New Variables
sex <- puffinbill$sex
curlen <- puffinbill$curlen
# Coding Female as 1 and Male as 0
sexcode <- ifelse(sex == "F", 1, 0)
# Plotting the Data
plot(curlen, jitter(x = sexcode, factor = 0.15), pch = 19,
xlab = "Curvature Length", ylab = "Sex (0 - Male, 1 - Female)")
# Logistic Regression Model
model <- glm(sexcode ~ curlen, binomial)
model
summary(model)
# Creating a sequence from minimum to maximum Curvature Length in steps of 0.01
xv <- seq(min(curlen), max(curlen), 0.01)
# Predicting the values for the given xv values
ypredict <- predict(model, list(curlen = xv), type = "response")
lines(xv, ypredict, col ="red")
# Some other plots
library(popbio)
logi.hist.plot(curlen, sexcode, boxp = F, type = "count", col = "gray",
xlabel = "size")
library(nnet)
# Read the data from CSV
data <- read.csv(file.choose(),header = T)
#Predictor Variables
Age <- data$Age
# REsponse Variable
choice <- data$Choice
library(popbio)
logi.hist.plot(data$Salary, data$Loan.approval, boxp = F, type = "count", col = "gray",
xlabel = "size")
logi.hist.plot(Age, data$Loan.approval, boxp = F, type = "count", col = "gray",
xlabel = "size")
plot(Age, jitter(x = data$Loan.approval, factor = 0.15), pch = 19,
xlab = "Curvature Length", ylab = "Sex (0 - Male, 1 - Female)")
str(data)
plot(data$Age, jitter(x = data$Loan.approval, factor = 0.15), pch = 19,
xlab = "Curvature Length", ylab = "Sex (0 - Male, 1 - Female)")
library(textir)
library(MASS)
data(fgl)
fgl
str(fgl$type)
#boxplot(RI ~ type, data = fgl)
par(mfrow = c(3,3), mai = c(.3,.6,.1,.1))
plot(RI ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Na ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Mg ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Al ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Si ~ type, data = fgl, col = c(grey(.2),2:6))
plot(K ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Ca ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Ba ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Fe ~ type, data = fgl, col = c(grey(.2),2:6))
## for illustration, consider the RIxAI plane
## use nt=200 training cases to find the nearest neighbors for
## the remaining 14 cases. These 14 cases become the evaluation
## (test, hold-out) cases
n = length(fgl$type)
nt = 200
set.seed(1)
# to make the calculations reproducible in repeated runs
train <- sample(1:n, nt)
## Standardization of the data is preferrable, especially if
## units of the features are quite different
## could do this from scratch by calculating the mean and
## standard deviation of each feature, and use those to standardize
## Even simpler, use the normalize function in the R package textir;
## it converts the data frame column to mean 0 and sd 1
x <- scale(fgl[,c(4,1)])
x[1:3,]
library(class)
nearest1 <- knn(train = x[train,], test = x[-train,], cl=fgl$type[train], k=1)
nearest5 <- knn(train = x[train,], test = x[-train,], cl=fgl$type[train], k=5)
data.frame(fgl$type[-train],nearest1,nearest5)
# plot to see how it worked on the training set
par(mfrow = c(1,2))
# plot for k=1 (single) nearest neighbor
plot(x[train,],col = fgl$type[train], cex = 0.8, main = "1-nearest neighbor")
points(x[-train],bg=nearest1,pch=21, col=grey(.9),cex = 1.25)
# plot for k=5 nearest neighbor
plot(x[train,],col = fgl$type[train], cex = 0.8, main = "5-nearest neighbor")
points(x[-train],bg=nearest5,pch=21, col=grey(.9),cex = 1.25)
legend("topright",legend = levels(fgl$type), fill=1:6, bty="n",cex=.75)
# calculate the proportion of correct classification on this one trainig set
ppcorn1 = 100*sum(fgl$type[-train]==nearest1)/(n-nt)
ppcorn5 = 100*sum(fgl$type[-train]==nearest5)/(n-nt)
ppcorn1
ppcorn5
str(fgl$type)
str(flg)
str(fgl)
data = Caravan
library(ISLR)
data = Caravan
str(data)
## we need to make sure that our data is scaled or standardized
# the first 2 variables have very different ranges, and that proves why we need to standardize.
var(data[,1])
var(data[,2])
?fgl
str(fgl)
x <- scale(fgl[,c(4,1)])
x[1:3,]
caret
library(caret)
library(MASS)
type = fgl[,10]
std_fgl = scale(fgl[,-10])
set.seed(1)
test = sample(1:214,170)
train = -test
training_data = std_fgl[train,]
testing_data = std_fgl[test,]
type = fgl[,10] # save the type column in a seperate variable
testing_y = type[test]
training_y = type[train]
library(FNN)
predicted_y = knn(training_data,testing_data,training_y,k=1)
head(predicted_y)
library(caret)
confusionMatrix(testing_y,predicted_y)
MSE = NULL
for(i in 1:7){
set.seed(1)
predicted_y = knn(training_data,testing_data,training_y,k=i)
MSE[i] = mean(predicted_y != testing_y)
print(i)
print(table(testing_y,predicted_y))
}
MSE
}
type = fgl[,10]
type
str(fgl)
str(fgl$type)
summary(fgl$type)
testing_y
training_y
library(MASS)
summary(fgl$type)
type = fgl[,10]
std_fgl = scale(fgl[,-10])
set.seed(1)
test = sample(1:214,170)
train = -test
training_data = std_fgl[train,]
testing_data = std_fgl[test,]
type = fgl[,10] # save the type column in a seperate variable
testing_y = type[test]
training_y = type[train]
library(FNN)
predicted_y = knn(training_data,testing_data,training_y,k=1)
head(predicted_y)
library(caret)
confusionMatrix(testing_y,predicted_y)
round(5.333,2)
summary(fgl$type)
type = fgl[,10]
std_fgl = scale(fgl[,-10])
set.seed(1)
test = sample(1:214,150)
train = -test
training_data = std_fgl[train,]
testing_data = std_fgl[test,]
type = fgl[,10] # save the type column in a seperate variable
testing_y = type[test]
training_y = type[train]
library(FNN)
predicted_y = knn(training_data,testing_data,training_y,k=1)
head(predicted_y)
library(caret)
confusionMatrix(testing_y,predicted_y)
library(MASS)
summary(fgl$type)
type = fgl[,10]
std_fgl = scale(fgl[,-10])
set.seed(1)
test = sample(1:214,150)
train = -test
training_data = std_fgl[train,]
testing_data = std_fgl[test,]
type = fgl[,10] # save the type column in a seperate variable
testing_y = type[test]
training_y = type[train]
library(FNN)
predicted_y = knn(training_data,testing_data,training_y,k=1)
head(predicted_y)
library(caret)
confusionMatrix(testing_y,predicted_y)
ME = NULL
for(i in 1:7){
set.seed(1)
predicted_y = knn(training_data,testing_data,training_y,k=i)
ME[i] = mean(predicted_y != testing_y)
print(i)
print(table(testing_y,predicted_y))
print (round(ME,3))
ME = NULL
cat("\n")
}
ME = NULL
for(i in 1:7){
set.seed(1)
predicted_y = knn(training_data,testing_data,training_y,k=i)
ME[i] = mean(predicted_y != testing_y)
print(i)
print(table(testing_y,predicted_y))
cat("\n")
}
print (round(ME,3))
library(textir)
library(MASS)
data(fgl)
?fgl
str(fgl$type)
str(fgl)
#boxplot(RI ~ type, data = fgl)
par(mfrow = c(3,3), mai = c(.3,.6,.1,.1))
plot(RI ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Na ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Mg ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Al ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Si ~ type, data = fgl, col = c(grey(.2),2:6))
plot(K ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Ca ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Ba ~ type, data = fgl, col = c(grey(.2),2:6))
plot(Fe ~ type, data = fgl, col = c(grey(.2),2:6))
## for illustration, consider the RIxAI plane
## use nt=200 training cases to find the nearest neighbors for
## the remaining 14 cases. These 14 cases become the evaluation
## (test, hold-out) cases
n = length(fgl$type)
nt = 200
set.seed(1)
# to make the calculations reproducible in repeated runs
train <- sample(1:n, nt)
## Standardization of the data is preferrable, especially if
## units of the features are quite different
## could do this from scratch by calculating the mean and
## standard deviation of each feature, and use those to standardize
## Even simpler, use the normalize function in the R package textir;
## it converts the data frame column to mean 0 and sd 1
x <- scale(fgl[,c(4,1)])
x[1:3,]
library(class)
nearest1 <- knn(train = x[train,], test = x[-train,], cl=fgl$type[train], k=1)
nearest5 <- knn(train = x[train,], test = x[-train,], cl=fgl$type[train], k=5)
data.frame(fgl$type[-train],nearest1,nearest5)
# plot to see how it worked on the training set
par(mfrow = c(1,2))
# plot for k=1 (single) nearest neighbor
plot(x[train,],col = fgl$type[train], cex = 0.8, main = "1-nearest neighbor")
points(x[-train],bg=nearest1,pch=21, col=grey(.9),cex = 1.25)
# plot for k=5 nearest neighbor
plot(x[train,],col = fgl$type[train], cex = 0.8, main = "5-nearest neighbor")
points(x[-train],bg=nearest5,pch=21, col=grey(.9),cex = 1.25)
legend("topright",legend = levels(fgl$type), fill=1:6, bty="n",cex=.75)
# calculate the proportion of correct classification on this one trainig set
ppcorn1 = 100*sum(fgl$type[-train]==nearest1)/(n-nt)
ppcorn5 = 100*sum(fgl$type[-train]==nearest5)/(n-nt)
ppcorn1
ppcorn5
library(MASS)
summary(fgl$type)
type = fgl[,10]
std_fgl = scale(fgl[,-10])
set.seed(1)
test = sample(1:214,150)
train = -test
training_data = std_fgl[train,]
testing_data = std_fgl[test,]
type = fgl[,10] # save the type column in a seperate variable
testing_y = type[test]
training_y = type[train]
library(FNN)
predicted_y = knn(training_data,testing_data,training_y,k=1)
head(predicted_y)
library(caret)
confusionMatrix(testing_y,predicted_y)
ME = NULL
for(i in 1:7){
set.seed(1)
predicted_y = knn(training_data,testing_data,training_y,k=i)
ME[i] = mean(predicted_y != testing_y)
print(i)
print(table(testing_y,predicted_y))
cat("\n")
}
print (round(ME,3))
min_error_rate = min(ME)
K = which(ME == min_error_rate)
print(min_error_rate)
print (K)
